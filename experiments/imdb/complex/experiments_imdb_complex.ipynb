{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Confounding on IMDb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from scipy.stats import norm\n",
    "from scipy.special import expit\n",
    "\n",
    "# ATE estimation\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from doubleml import DoubleMLData, DoubleMLPLR\n",
    "from causalml.inference.meta import BaseSRegressor\n",
    "\n",
    "# Custom modules\n",
    "from utils.project import set_root\n",
    "from utils.io import save_results, load_results\n",
    "from visualization.plotting import plot_ate_estimates\n",
    "from helpers.ae import fit_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory and directory for saving results\n",
    "set_root()\n",
    "results_dir = \"results/comparison_learners/imdb/complex\"\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"densenet121-res224-all\"  # Pretrained model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "file_path = \"data/imdb/imdb_with_hidden_states_sentiment.csv\"\n",
    "df_imdb_sent_prepro = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the 'hidden_state' column from string to actual lists\n",
    "df_imdb_sent_prepro['hidden_state'] = df_imdb_sent_prepro['hidden_state'].apply(ast.literal_eval)\n",
    "\n",
    "# Expand the 'hidden_state' column into separate columns\n",
    "hidden_state_df = pd.DataFrame(df_imdb_sent_prepro['hidden_state'].tolist())\n",
    "\n",
    "# Rename the hidden state columns\n",
    "hidden_state_df.columns = [f\"hidden_state_{i}\" for i in range(hidden_state_df.shape[1])]\n",
    "\n",
    "# Concatenate the expanded hidden states with the original DataFrame (excluding original 'hidden_state' column)\n",
    "df_imdb_prepro = pd.concat([df_imdb_sent_prepro.drop(columns=['hidden_state']), hidden_state_df], axis=1)\n",
    "\n",
    "# Drop the 'text' column containing the review text\n",
    "df_imdb = df_imdb_prepro.drop(columns=['text'])\n",
    "\n",
    "# Depict the first few rows of the DataFrame\n",
    "df_imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further preprcessing the dataset for the simulation\n",
    "\n",
    "# Subsample the dataset to a manageable size for simulation\n",
    "n = 4000  # Number of samples to draw\n",
    "df_imdb_sampled = df_imdb.sample(n=n, replace=False, random_state=42) \n",
    "\n",
    "# Extract sentiment labels and latent representations\n",
    "sent_label = df_imdb_sampled['label']\n",
    "imdb_latent_rep = df_imdb_sampled.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Training on Pre-trained Representations (Used for Complex Confounding Simuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an autoencoder on the pre-trained representations (latent_features of the AE will be used to simulate confounding)\n",
    "latent_dim = 5 # Dimensionality of the latent space\n",
    "encoder, latent_features, scaler = fit_autoencoder(imdb_latent_rep, latent_dim=latent_dim, epochs=50, batch_size=32, random_seed=42)\n",
    "\n",
    "# Obtain scaled imdb pre-trained representations \n",
    "imdb_latent_rep_scaled = scaler.transform(imdb_latent_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Confounding Simulation and ATE Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Simulation Parameters\n",
    "np.random.seed(42)  # For reproducibility\n",
    "beta_true = 2.0     # True effect of A on Y\n",
    "gammas_Y = np.random.uniform(-1, 0, latent_dim)   # Effect latent feature confounder on Y\n",
    "gamma_A = np.random.normal(0, 1, latent_dim)  # Effect latent feature confounder on A\n",
    "\n",
    "# 2. Specify general parameters for simulation\n",
    "n_samples = latent_features.shape[0]\n",
    "d_features = imdb_latent_rep_scaled.shape[1]\n",
    "n_runs = 5  # Number of simulation runs\n",
    "ci_alpha_level = 0.05  # Alpha level for 1-alpha confidence intervals\n",
    "z_score = norm.ppf(1 - ci_alpha_level / 2) # Z-score for 1-alpha confidence intervals\n",
    "\n",
    "# 3. Initialize Storage for Estimates and Confidence Intervals\n",
    "methods = ['Naive', 'S-Learner (NN)', 'S-Learner (RF)', 'DML (NN)', 'DML (RF)'] \n",
    "\n",
    "estimates_dict_complex = {method: [] for method in methods}\n",
    "cis_dict_complex = {method: {'lower': [], 'upper': []} for method in methods}\n",
    "\n",
    "# 4. Simulation Loop\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n--- Simulation Run {run + 1} ---\")\n",
    "    # Set a unique seed for each run for variability\n",
    "    seed = 42 + run\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # a. Compute Treatment Assignment Probability\n",
    "    pA = expit(latent_features @ gamma_A)\n",
    "\n",
    "    # b. Sample Treatment Assignment\n",
    "    A = np.random.binomial(1, pA)\n",
    "\n",
    "    # c. Generate Outcome Y\n",
    "    noise = np.random.normal(loc=0, scale=1, size=n_samples)\n",
    "    Y = beta_true * A + latent_features @ gammas_Y + noise\n",
    "    \n",
    "    # d. Package into DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Y': Y,\n",
    "        'A': A\n",
    "    })\n",
    "    \n",
    "    # 4.1. Naive OLS (Unadjusted) using statsmodels\n",
    "    X_naive = sm.add_constant(df['A']) \n",
    "    model_naive = sm.OLS(df['Y'], X_naive).fit()\n",
    "    beta_naive = model_naive.params['A']\n",
    "    se_naive = model_naive.bse['A']\n",
    "    ci_lower_naive = beta_naive - z_score * se_naive\n",
    "    ci_upper_naive = beta_naive + z_score * se_naive\n",
    "    estimates_dict_complex['Naive'].append(beta_naive)\n",
    "    cis_dict_complex['Naive']['lower'].append(ci_lower_naive)\n",
    "    cis_dict_complex['Naive']['upper'].append(ci_upper_naive)\n",
    "    print(f\"Naive OLS: β = {beta_naive:.3f}, SE = {se_naive:.3f}\")\n",
    "    \n",
    "    ## Different Other ATE Estimators\n",
    "\n",
    "    # 4.2.1. S-Learner (NN)\n",
    "    outcome_model_nn = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('mlp', MLPRegressor(\n",
    "            hidden_layer_sizes=(100, 50),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            max_iter=500,\n",
    "            random_state=seed\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        s_learner_nn = BaseSRegressor(outcome_model_nn) \n",
    "        s_ate_nn, s_ci_lower_nn, s_ci_upper_nn = s_learner_nn.estimate_ate(imdb_latent_rep_scaled, A, Y, return_ci=True)\n",
    "        estimates_dict_complex['S-Learner (NN)'].append(s_ate_nn[0])\n",
    "        cis_dict_complex['S-Learner (NN)']['lower'].append(s_ci_lower_nn[0])\n",
    "        cis_dict_complex['S-Learner (NN)']['upper'].append(s_ci_upper_nn[0])\n",
    "        print(f\"S-Learner (NN): β = {s_ate_nn[0]:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Run {run+1}: S-Learner (NN) failed with error: {e}\")\n",
    "        estimates_dict_complex['S-Learner (NN)'].append(np.nan)\n",
    "        cis_dict_complex['S-Learner (NN)']['lower'].append(np.nan)\n",
    "        cis_dict_complex['S-Learner (NN)']['upper'].append(np.nan)\n",
    "    \n",
    "    # 4.2.2. S-Learner (RF) \n",
    "    outcome_model_rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "    try:\n",
    "        s_learner_rf = BaseSRegressor(outcome_model_rf) \n",
    "        s_ate_rf, s_ci_lower_rf, s_ci_upper_rf = s_learner_rf.estimate_ate(imdb_latent_rep_scaled, A, Y, return_ci=True)\n",
    "        estimates_dict_complex['S-Learner (RF)'].append(s_ate_rf[0])\n",
    "        cis_dict_complex['S-Learner (RF)']['lower'].append(s_ci_lower_rf[0])\n",
    "        cis_dict_complex['S-Learner (RF)']['upper'].append(s_ci_upper_rf[0])\n",
    "        print(f\"S-Learner (RF): β = {s_ate_rf[0]:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Run {run+1}: S-Learner (RF) failed with error: {e}\")\n",
    "        estimates_dict_complex['S-Learner (RF)'].append(np.nan)\n",
    "        cis_dict_complex['S-Learner (RF)']['lower'].append(np.nan)\n",
    "        cis_dict_complex['S-Learner (RF)']['upper'].append(np.nan)\n",
    "\n",
    "    # 4.3. DoubleML with Neural Network Nuisance and Random Forest Estimators\n",
    "    # Convert imdb_latent_rep to DataFrame\n",
    "    X_dml_df = pd.DataFrame(\n",
    "        imdb_latent_rep_scaled,  # Use scaled features\n",
    "        columns=[f\"feat_{i}\" for i in range(d_features)]\n",
    "    )\n",
    "    \n",
    "    # Add outcome and treatment to DoubleMLData via column names\n",
    "    X_dml_df['Y'] = df['Y']\n",
    "    X_dml_df['A'] = df['A']\n",
    "\n",
    "    # Create DoubleMLData\n",
    "    data_dml = DoubleMLData(X_dml_df, \"Y\", \"A\")\n",
    "\n",
    "    # 4.3.1. DoubleML with Neural Network Nuisance Estimators\n",
    "    try:\n",
    "        # Define nuisance models with a neural network and logistic regression\n",
    "        ml_g_nn = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('mlp', MLPRegressor(\n",
    "                hidden_layer_sizes=(100, 50),\n",
    "                activation='relu',\n",
    "                solver='adam',\n",
    "                max_iter=500,\n",
    "                random_state=seed\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        ml_m_log = LogisticRegression()\n",
    "    \n",
    "        # Instantiate and fit DoubleMLPLR\n",
    "        dml_plr_nn = DoubleMLPLR(data_dml, ml_g_nn, ml_m_log, n_folds=2)\n",
    "        dml_plr_nn.fit()\n",
    "        beta_dml_nn = dml_plr_nn.coef[0]\n",
    "        se_dml_nn = dml_plr_nn.se[0]\n",
    "        estimates_dict_complex['DML (NN)'].append(beta_dml_nn)\n",
    "        # 95% Confidence Interval\n",
    "        ci_lower_dml_nn = beta_dml_nn - z_score * se_dml_nn\n",
    "        ci_upper_dml_nn = beta_dml_nn + z_score * se_dml_nn\n",
    "        cis_dict_complex['DML (NN)']['lower'].append(ci_lower_dml_nn)\n",
    "        cis_dict_complex['DML (NN)']['upper'].append(ci_upper_dml_nn)\n",
    "        print(f\"DML (NN): β = {beta_dml_nn:.3f}, SE = {se_dml_nn:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Run {run+1}: DML (NN) failed with error: {e}\")\n",
    "        estimates_dict_complex['DML'].append(np.nan)\n",
    "        cis_dict_complex['DML (NN)']['lower'].append(np.nan)\n",
    "        cis_dict_complex['DML (NN)']['upper'].append(np.nan)\n",
    "\n",
    "    # 4.3.2. DoubleML with Random Forest Nuisance Estimators\n",
    "\n",
    "    try:\n",
    "        # Define nuisance models with neural networks\n",
    "        ml_g_rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "        ml_m_rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    \n",
    "        # Instantiate and fit DoubleMLPLR\n",
    "        dml_plr_rf = DoubleMLPLR(data_dml, ml_g_rf, ml_m_rf, n_folds=2) \n",
    "        dml_plr_rf.fit()\n",
    "        beta_dml_rf = dml_plr_rf.coef[0]\n",
    "        se_dml_rf = dml_plr_rf.se[0]\n",
    "        estimates_dict_complex['DML (RF)'].append(beta_dml_rf)\n",
    "        # 95% Confidence Interval\n",
    "        ci_lower_dml_rf = beta_dml_rf - z_score * se_dml_rf\n",
    "        ci_upper_dml_rf = beta_dml_rf + z_score * se_dml_rf\n",
    "        cis_dict_complex['DML (RF)']['lower'].append(ci_lower_dml_rf)\n",
    "        cis_dict_complex['DML (RF)']['upper'].append(ci_upper_dml_rf)\n",
    "        print(f\"DML (RF): β = {beta_dml_rf:.3f}, SE = {se_dml_rf:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Run {run+1}: DML failed with error: {e}\")\n",
    "        estimates_dict_complex['DML (RF)'].append(np.nan)\n",
    "        cis_dict_complex['DML (RF)']['lower'].append(np.nan)\n",
    "        cis_dict_complex['DML (RF)']['upper'].append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create a directory for the experiment and save the results\n",
    "experiment_name = \"exp_results\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_dir = os.path.join(results_dir, model_name, experiment_name, timestamp)\n",
    "save_results(experiment_dir, estimates_dict_complex, cis_dict_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results from the previous experiment\n",
    "estimates_dict_complex, cis_dict_complex = load_results(experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ATE estimates with confidence intervals\n",
    "plot_ate_estimates(\n",
    "    estimates_dict=estimates_dict_complex,\n",
    "    cis_dict=cis_dict_complex,\n",
    "    plot_name=\"ate_estimates_complex_conf_imdb\",\n",
    "    save_dir=experiment_dir,\n",
    "    ate_true=2.0,\n",
    "    n_runs=5,\n",
    "    vert_diff=0.1,\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pretrained-causal-adj)",
   "language": "python",
   "name": "pretrained-causal-adj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
