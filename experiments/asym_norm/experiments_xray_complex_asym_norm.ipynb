{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Confounding on X-ray data - Asymptotic Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from scipy.special import expit\n",
    "\n",
    "# ATE estimation\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from doubleml import DoubleMLData, DoubleMLPLR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from causalml.inference.meta import BaseSRegressor\n",
    "\n",
    "# Custom modules\n",
    "from feature_extraction.pretrained_models_xrv import load_torchxrayvision_model, extract_features_from_folder\n",
    "from utils.project import set_root\n",
    "from utils.io import save_results, load_results\n",
    "from helpers.ae import fit_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory, dataset directory and directory for saving results\n",
    "set_root()\n",
    "dataset_dir = \"data/xray/raw/all_unique\"\n",
    "results_dir = \"results/asym_normality/xray/complex\"\n",
    "\n",
    "# Define the model name and path for saving results\n",
    "model_name = \"densenet121-res224-all\"  # Pretrained model name\n",
    "save_dir = f\"data/xray/representations/{model_name}\"\n",
    "\n",
    "# Define file paths\n",
    "features_path = os.path.join(save_dir, \"latent_features.npy\")\n",
    "labels_path = os.path.join(save_dir, \"labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction and saving (Only if the features and labels do not already exist)\n",
    "if not os.path.exists(features_path) or not os.path.exists(labels_path):\n",
    "    print(f\"Extracting features using model '{model_name}'...\")\n",
    "    \n",
    "    # Extract features and save them\n",
    "    model = load_torchxrayvision_model(model_name)\n",
    "    all_features, labels = extract_features_from_folder(\n",
    "        dataset_dir,\n",
    "        model,\n",
    "        device='cpu',\n",
    "        batch_size=32,\n",
    "        save_path=save_dir\n",
    "    )\n",
    "    \n",
    "    print(f\"Features extracted and saved to: {save_dir}\")\n",
    "else:\n",
    "    print(f\"Features already exist in {save_dir}. Skipping extraction.\")\n",
    "\n",
    "# Load extracted features\n",
    "all_features = np.load(features_path)\n",
    "labels = np.load(labels_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder Training on Pre-trained Representations (Used for Complex Confounding Simuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an autoencoder on the pre-trained representations (latent_features of the AE will be used to simulate confounding)\n",
    "latent_dim = 5 # Dimensionality of the latent space\n",
    "encoder, latent_features, scaler = fit_autoencoder(all_features, latent_dim=latent_dim, epochs=50, batch_size=32)\n",
    "\n",
    "# Obtain scaled imdb pre-trained representations \n",
    "all_features_scaled = scaler.transform(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Confounding Simulation and ATE Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define simulation parameters\n",
    "beta_true = 2.0     # True effect of A on Y\n",
    "np.random.seed(42)  # For reproducibility\n",
    "gammas_Y = np.random.uniform(-1, 0, latent_dim)   # Effect latent feature confounder on Y\n",
    "gamma_A = np.random.normal(0, 1, latent_dim)  # Effect latent feature confounder on A\n",
    "\n",
    "# 2. Specify general parameters for simulation\n",
    "n_samples = latent_features.shape[0]\n",
    "d_features = all_features_scaled.shape[1]\n",
    "n_runs = 200  # Number of simulation runs\n",
    "ci_alpha_level = 0.05  # Alpha level for 1-alpha confidence intervals\n",
    "z_score = norm.ppf(1 - ci_alpha_level / 2) # Z-score for 1-alpha confidence intervals\n",
    "\n",
    "# 3. Initialize Storage for Estimates and Confidence Intervals\n",
    "methods = ['Naive', 'S-Learner (NN)', 'DML (NN)']  \n",
    "\n",
    "estimates_dict_complex = {method: [] for method in methods}\n",
    "cis_dict_complex = {method: {'se': [], 'lower': [], 'upper': []} for method in methods}\n",
    "\n",
    "# 4. Simulation Loop\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n--- Simulation Run {run + 1} ---\")\n",
    "    # Set a unique seed for each run for variability\n",
    "    seed = 42 + run\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # a. Compute Treatment Assignment Probability\n",
    "    pA = expit(latent_features @ gamma_A)\n",
    "\n",
    "    # b. Sample Treatment Assignment\n",
    "    A = np.random.binomial(1, pA)\n",
    "\n",
    "    # c. Generate Outcome Y\n",
    "    noise = np.random.normal(loc=0, scale=1, size=n_samples)\n",
    "    Y = beta_true * A + latent_features @ gammas_Y + noise\n",
    "    \n",
    "    # d. Package into DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Y': Y,\n",
    "        'A': A\n",
    "    })\n",
    "    \n",
    "    # 4.1. Naive OLS (Unadjusted) using statsmodels\n",
    "    X_naive = sm.add_constant(df['A']) \n",
    "    model_naive = sm.OLS(df['Y'], X_naive).fit()\n",
    "    beta_naive = model_naive.params['A']\n",
    "    se_naive = model_naive.bse['A']\n",
    "    ci_lower_naive = beta_naive - z_score * se_naive\n",
    "    ci_upper_naive = beta_naive + z_score * se_naive\n",
    "    estimates_dict_complex['Naive'].append(beta_naive)\n",
    "    cis_dict_complex['Naive']['se'].append(se_naive)\n",
    "    cis_dict_complex['Naive']['lower'].append(ci_lower_naive)\n",
    "    cis_dict_complex['Naive']['upper'].append(ci_upper_naive)\n",
    "    print(f\"Naive OLS: β = {beta_naive:.3f}, SE = {se_naive:.3f}\")\n",
    "    \n",
    "    ## Different Other ATE Estimators\n",
    "\n",
    "    # 4.2. S-Learner (NN)\n",
    "    outcome_model_nn = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('mlp', MLPRegressor(\n",
    "            hidden_layer_sizes=(100, 50),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            max_iter=500,\n",
    "            random_state=seed\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        s_learner_nn = BaseSRegressor(outcome_model_nn) \n",
    "        s_ate_nn, s_ci_lower_nn, s_ci_upper_nn = s_learner_nn.estimate_ate(all_features_scaled, A, Y, return_ci=True)\n",
    "        estimates_dict_complex['S-Learner (NN)'].append(s_ate_nn[0])\n",
    "        se_slearner_nn = (s_ci_upper_nn[0] - s_ate_nn[0]) / z_score\n",
    "        cis_dict_complex['S-Learner (NN)']['se'].append(se_slearner_nn)\n",
    "        cis_dict_complex['S-Learner (NN)']['lower'].append(s_ci_lower_nn[0])\n",
    "        cis_dict_complex['S-Learner (NN)']['upper'].append(s_ci_upper_nn[0])\n",
    "        print(f\"S-Learner (NN): β = {s_ate_nn[0]:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Run {run+1}: S-Learner (NN) failed with error: {e}\")\n",
    "        estimates_dict_complex['S-Learner (NN)'].append(np.nan)\n",
    "        cis_dict_complex['S-Learner (NN)']['lower'].append(np.nan)\n",
    "        cis_dict_complex['S-Learner (NN)']['upper'].append(np.nan)\n",
    "\n",
    "\n",
    "    # 4.3. DoubleML with Neural Network Nuisance\n",
    "    # Convert all_features to DataFrame\n",
    "    X_dml_df = pd.DataFrame(\n",
    "        all_features_scaled,  # Use scaled features\n",
    "        columns=[f\"feat_{i}\" for i in range(d_features)]\n",
    "    )\n",
    "    \n",
    "    # Add outcome and treatment to DoubleMLData via column names\n",
    "    X_dml_df['Y'] = df['Y']\n",
    "    X_dml_df['A'] = df['A']\n",
    "\n",
    "    # Create DoubleMLData\n",
    "    data_dml = DoubleMLData(X_dml_df, \"Y\", \"A\")\n",
    "\n",
    "    # 4.3.1. DoubleML with Neural Network Nuisance Estimators\n",
    "    try:\n",
    "        # Define nuisance models with neural networks\n",
    "        ml_g_nn = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('mlp', MLPRegressor(\n",
    "                hidden_layer_sizes=(100, 50),\n",
    "                activation='relu',\n",
    "                solver='adam',\n",
    "                max_iter=500,\n",
    "                random_state=seed\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        ml_m_log = LogisticRegression()\n",
    "    \n",
    "        # Instantiate and fit DoubleMLPLR\n",
    "        dml_plr_nn = DoubleMLPLR(data_dml, ml_g_nn, ml_m_log, n_folds=2)\n",
    "        dml_plr_nn.fit()\n",
    "        beta_dml_nn = dml_plr_nn.coef[0]\n",
    "        se_dml_nn = dml_plr_nn.se[0]\n",
    "        estimates_dict_complex['DML (NN)'].append(beta_dml_nn)\n",
    "        # 95% Confidence Interval\n",
    "        ci_lower_dml_nn = beta_dml_nn - z_score * se_dml_nn\n",
    "        ci_upper_dml_nn = beta_dml_nn + z_score * se_dml_nn\n",
    "        cis_dict_complex['DML (NN)']['se'].append(se_dml_nn)\n",
    "        cis_dict_complex['DML (NN)']['lower'].append(ci_lower_dml_nn)\n",
    "        cis_dict_complex['DML (NN)']['upper'].append(ci_upper_dml_nn)\n",
    "        print(f\"DML (NN): β = {beta_dml_nn:.3f}, SE = {se_dml_nn:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Run {run+1}: DML (NN) failed with error: {e}\")\n",
    "        estimates_dict_complex['DML (NN)'].append(np.nan)\n",
    "        cis_dict_complex['DML (NN)']['se'].append(np.nan)\n",
    "        cis_dict_complex['DML (NN)']['lower'].append(np.nan)\n",
    "        cis_dict_complex['DML (NN)']['upper'].append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create a directory for the experiment and save the results\n",
    "experiment_name = \"exp_results\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_dir = os.path.join(results_dir, model_name, experiment_name, timestamp)\n",
    "save_results(experiment_dir, estimates_dict_complex, cis_dict_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results from the previous experiment\n",
    "estimates_dict_complex, cis_dict_complex = load_results(experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the results\n",
    "dml_est_complex_normalized = (np.array(estimates_dict_complex['DML (NN)']) - beta_true) / np.array(cis_dict_complex['DML (NN)']['se'])\n",
    "slearner_est_complex_normalized = (np.array(estimates_dict_complex['S-Learner (NN)']) - beta_true) / np.array(cis_dict_complex['S-Learner (NN)']['se'])\n",
    "naive_est_complex_normalized = (np.array(estimates_dict_complex['Naive']) - beta_true) / np.array(cis_dict_complex['Naive']['se'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Plotting\n",
    "sns.set(style=\"whitegrid\")\n",
    "face_colors = sns.color_palette('pastel')\n",
    "edge_colors = sns.color_palette('dark')\n",
    "\n",
    "fig_orth_nosplit, ax = plt.subplots(constrained_layout=True)\n",
    "\n",
    "n_bins = 30\n",
    "sns.histplot(naive_est_complex_normalized,\n",
    "                color=face_colors[1], edgecolor = edge_colors[1],\n",
    "                stat='density', bins=8, label='Naive')\n",
    "sns.histplot(slearner_est_complex_normalized,\n",
    "                color=face_colors[0], edgecolor = edge_colors[0],\n",
    "                stat='density', bins=n_bins, label='S-Learner')\n",
    "sns.histplot(dml_est_complex_normalized,\n",
    "                color=face_colors[2], edgecolor = edge_colors[2],\n",
    "                stat='density', bins=n_bins, label='DML')\n",
    "x_val_norm = np.arange(-4, 4, 0.001)\n",
    "y_val_norm = norm.pdf(x_val_norm)\n",
    "ax.plot(x_val_norm, y_val_norm, color='k', label='$\\\\mathcal{N}(0, 1)$', linewidth=1.5)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "ax.set_xlim([-40, 20])\n",
    "ax.set_xlabel('$(\\widehat{ATE} - ATE)/\\hat{\\sigma}$')\n",
    "plot_path = os.path.join(experiment_dir, 'ate_asym_norm_complex_conf_pneu.pdf')\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pretrained-causal-adj)",
   "language": "python",
   "name": "pretrained-causal-adj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
